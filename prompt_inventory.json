[
  {
    "file": "aeon/plan/prompts.py",
    "function": "get_plan_generation_system_prompt",
    "prompt_type": "system prompt",
    "category": "plan generation",
    "exact_string": "You are a planning assistant. Generate declarative plans in JSON format.\nA plan must have:\n- \"goal\": string describing the objective\n- \"steps\": array of step objects, each with:\n  - \"step_id\": unique identifier (string)\n  - \"description\": what the step does (string)\n  - \"status\": \"pending\" (always pending for new plans)\n  - \"tool\": (optional) name of registered tool for tool-based execution\n  - \"agent\": (optional) \"llm\" for explicit LLM reasoning steps\n\nIMPORTANT: Only reference tools that exist in the available tools list. Do not invent tools.\nIf a step requires a tool, use the \"tool\" field with the exact tool name from the available list.\nIf a step requires reasoning without a tool, use \"agent\": \"llm\".\n\nReturn only valid JSON.",
    "is_reused": true,
    "reused_in": ["aeon/plan/recursive.py::generate_plan", "aeon/plan/recursive.py::create_subplan", "aeon/kernel/orchestrator.py::generate_plan"],
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "Reused by RecursivePlanner for plan generation and subplan creation, and by Orchestrator for initial plan generation"
  },
  {
    "file": "aeon/plan/prompts.py",
    "function": "construct_plan_generation_prompt",
    "prompt_type": "user template",
    "category": "plan generation",
    "exact_string": "Generate a plan to accomplish the following request:\n\n{request}\n\n\n[Conditional: if tool_registry provided]\nAvailable tools:\n- {tool_name}: {tool_description}\n  Input schema: {tool_input_schema}\n\nYou may reference these tools in step.tool fields. Do not invent tools.\n\n[End conditional]\nReturn a JSON plan with goal and steps.",
    "is_reused": true,
    "reused_in": ["aeon/kernel/orchestrator.py::generate_plan", "aeon/plan/recursive.py::generate_plan"],
    "is_parameterized": true,
    "parameters": ["request", "tool_registry (optional)"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Template with conditional tool registry inclusion. Tool list is dynamically built from tool_registry.export_tools_for_llm()"
  },
  {
    "file": "aeon/plan/prompts.py",
    "function": "build_reasoning_prompt",
    "prompt_type": "user template",
    "category": "execution reasoning prompt",
    "exact_string": "Task: {step.description}\n\n[Conditional: if phase_context provided]\n=== Execution Context ===\nRequest: {phase_context['request']}\nPlan Goal: {plan_goal}\nPass Number: {phase_context['pass_number']}\nPhase: {phase_context['phase']}\nTTL Remaining: {phase_context['ttl_remaining']}\nCorrelation ID: {phase_context['correlation_id']}\n[Conditional: if previous_outputs]\nPrevious Outputs: {len(phase_context['previous_outputs'])} result(s)\n[End conditional]\n[Conditional: if refinement_changes]\nRefinement Changes: {len(phase_context['refinement_changes'])} change(s)\n[End conditional]\n\n[End conditional]\n[Conditional: if step.step_index]\nStep {step.step_index}[Conditional: if step.total_steps] of {step.total_steps}[End conditional]\n\n[End conditional]\n[Conditional: if step.incoming_context]\nContext from previous steps:\n{step.incoming_context}\n\n[End conditional]\n[Conditional: if memory.search('step_') returns results]\nAdditional context from memory:\n- {key}: {value}\n[End conditional]\n\nPlease provide your reasoning and result. If the task is clear and you can proceed, respond with clarity_state: CLEAR. If the task is partially clear but needs more information, respond with clarity_state: PARTIALLY_CLEAR. If the task is blocked and cannot proceed, respond with clarity_state: BLOCKED. Include a 'handoff_to_next' field with context to pass to the next step.",
    "is_reused": true,
    "reused_in": ["aeon/kernel/executor.py::execute_llm_reasoning_step"],
    "is_parameterized": true,
    "parameters": ["step", "memory", "phase_context (optional)"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Complex parameterized template with multiple conditional sections for phase context, step context, incoming context, and memory context"
  },
  {
    "file": "aeon/supervisor/repair.py",
    "function": "_get_default_system_prompt",
    "prompt_type": "system prompt",
    "category": "supervisor repair prompt (json/tool/plan/missing)",
    "exact_string": "You are a JSON repair assistant. Fix malformed JSON, tool calls, or plan structures.\nReturn only the corrected JSON. Do not add new fields, tools, or semantic meaning.\nYour job is to correct syntax and structure only.",
    "is_reused": true,
    "reused_in": ["aeon/supervisor/repair.py::repair_json", "aeon/supervisor/repair.py::repair_tool_call", "aeon/supervisor/repair.py::repair_plan", "aeon/supervisor/repair.py::repair_missing_tool_step"],
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "Default system prompt for all supervisor repair operations. Can be overridden via constructor parameter."
  },
  {
    "file": "aeon/supervisor/repair.py",
    "function": "_construct_json_repair_prompt",
    "prompt_type": "user template",
    "category": "supervisor repair prompt (json/tool/plan/missing)",
    "exact_string": "Fix this malformed JSON: {malformed_json}\n\n[Conditional: if expected_schema provided]\nExpected schema: {json.dumps(expected_schema, indent=2)}\n[End conditional]\n\nReturn only the corrected JSON, no explanation.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["malformed_json", "expected_schema (optional)"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for JSON repair operations"
  },
  {
    "file": "aeon/supervisor/repair.py",
    "function": "_construct_tool_call_repair_prompt",
    "prompt_type": "user template",
    "category": "supervisor repair prompt (json/tool/plan/missing)",
    "exact_string": "Fix this malformed tool call: {json.dumps(malformed_call, indent=2)}\n\nTool schema: {json.dumps(tool_schema, indent=2)}\n\nReturn only the corrected tool call JSON, no explanation.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["malformed_call", "tool_schema"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for tool call repair operations"
  },
  {
    "file": "aeon/supervisor/repair.py",
    "function": "_construct_plan_repair_prompt",
    "prompt_type": "user template",
    "category": "supervisor repair prompt (json/tool/plan/missing)",
    "exact_string": "Fix this malformed plan: {json.dumps(malformed_plan, indent=2)}\n\nA valid plan must have:\n- \"goal\": string\n- \"steps\": array of step objects with \"step_id\", \"description\", \"status\"\n\nReturn only the corrected plan JSON, no explanation.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["malformed_plan"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for plan structure repair operations"
  },
  {
    "file": "aeon/supervisor/repair.py",
    "function": "_construct_missing_tool_repair_prompt",
    "prompt_type": "user template",
    "category": "supervisor repair prompt (json/tool/plan/missing)",
    "exact_string": "Repair this step to reference a valid tool from the available list. Do not invent tools.\n\nPlan goal: {plan_goal}\n\nStep to repair:\n{json.dumps(step_context, indent=2)}\n\n{tools_text}\n\nReturn only the corrected step JSON with a valid tool reference from the available tools list.\nThe step must have: step_id, description, and tool (referencing one of the available tools).\nClear any errors field on successful repair.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["step", "available_tools", "plan_goal"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for missing-tool step repair operations. tools_text is dynamically built from available_tools list."
  },
  {
    "file": "aeon/plan/recursive.py",
    "function": "generate_plan",
    "prompt_type": "user template",
    "category": "refinement/subplan/recursive",
    "exact_string": "Generate a plan to accomplish the following request:\n\n{task_description}\n\n\nTaskProfile context:\n- Reasoning depth: {task_profile.reasoning_depth}/5\n- Information sufficiency: {task_profile.information_sufficiency:.2f}\n- Expected tool usage: {task_profile.expected_tool_usage}\n- Output breadth: {task_profile.output_breadth}\n- Confidence requirement: {task_profile.confidence_requirement}\n\nGenerate a plan with steps that include step_index, total_steps, incoming_context, and handoff_to_next fields.\n",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["task_description", "task_profile", "tool_registry"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Enhanced plan generation with TaskProfile context. Uses construct_plan_generation_prompt() as base and appends TaskProfile context."
  },
  {
    "file": "aeon/plan/recursive.py",
    "function": "create_subplan",
    "prompt_type": "user template",
    "category": "refinement/subplan/recursive",
    "exact_string": "Generate a subplan to decompose the following step:\n\nStep ID: {parent_step_id}\nStep Description: {parent_step_description}\nNesting Depth: {current_depth + 1} (max: {max_depth})\n\nBreak this step down into detailed substeps. Each substep should be concrete and actionable.\nReturn a JSON plan with goal and steps.\n",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["parent_step_id", "parent_step_description", "current_depth", "max_depth"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for recursive subplan generation"
  },
  {
    "file": "aeon/plan/recursive.py",
    "function": "_generate_refinement_actions_llm",
    "prompt_type": "system prompt",
    "category": "refinement/subplan/recursive",
    "exact_string": "You are a plan refinement assistant. Generate refinement actions as JSON array.",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "System prompt for plan refinement"
  },
  {
    "file": "aeon/plan/recursive.py",
    "function": "_generate_refinement_actions_llm",
    "prompt_type": "user template",
    "category": "refinement/subplan/recursive",
    "exact_string": "Generate refinement actions for the following plan:\n\nCurrent Plan:\nGoal: {current_plan.goal}\nSteps:\n{step_list_with_execution_markers}\n\nRefinement Triggers:\n{trigger_list}\n\nTarget Fragments (can be refined): {target_fragments}\n\nGenerate refinement actions (ADD/MODIFY/REMOVE/REPLACE) as JSON array.\nEach action should have: action_type, target_step_id (or target_plan_section), new_step (for ADD/MODIFY), changes, reason, semantic_validation_input, inconsistency_detected.\n",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["current_plan", "target_fragments", "refinement_triggers", "executed_step_ids"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Used for plan refinement action generation. step_list includes [EXECUTED - DO NOT MODIFY] markers for executed steps."
  },
  {
    "file": "aeon/validation/semantic.py",
    "function": "_get_system_prompt",
    "prompt_type": "system prompt",
    "category": "validation semantic prompt",
    "exact_string": "You are a semantic validation assistant. Analyze plans, steps, and execution artifacts for quality issues.\nIdentify specificity problems, relevance issues, do/say mismatches, hallucinated tools, and consistency violations.\nClassify issues by type and assign severity scores. Propose semantic repairs when possible.\nReturn structured JSON with detected issues.",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "System prompt for semantic validation"
  },
  {
    "file": "aeon/validation/semantic.py",
    "function": "_construct_validation_prompt",
    "prompt_type": "user template",
    "category": "validation semantic prompt",
    "exact_string": "[Varies by artifact_type]\n\nFor artifact_type='plan':\nValidate this plan for semantic quality issues:\nGoal: {artifact.get('goal', 'N/A')}\nSteps: {json.dumps(artifact.get('steps', []), indent=2)}\n\nCheck for the following issues:\n1. SPECIFICITY: Are steps concrete and actionable? Are descriptions vague or unclear?\n2. RELEVANCE: Do steps contribute to the overall goal? Are there irrelevant steps?\n3. DO/SAY MISMATCH: Do step descriptions match their actions or tool invocations?\n4. CONSISTENCY: Do steps logically flow? Are there circular dependencies? Are dependencies satisfied?\n[Conditional: if tool_registry]\nAvailable tools: {tool_names}\n5. HALLUCINATION: Are any tools referenced that don't exist in the available tools list?\n[End conditional]\n\nFor artifact_type='step':\nValidate this step for semantic quality issues:\n{json.dumps(artifact, indent=2)}\n\nCheck for the following issues:\n1. SPECIFICITY: Is the step description concrete and actionable?\n2. RELEVANCE: Does this step contribute to the plan goal?\n3. DO/SAY MISMATCH: Does the step description match its actions or tool invocations?\n[Conditional: if tool_registry and artifact.get('tool')]\nAvailable tools: {tool_names}\n4. HALLUCINATION: Is the tool '{artifact.get('tool')}' in the available tools list?\n[End conditional]\n\nFor artifact_type='execution_artifact':\nValidate this execution artifact for semantic quality issues:\n{json.dumps(artifact, indent=2)}\n\nCheck for the following issues:\n1. CONSISTENCY: Does the artifact align with the plan and step descriptions?\n2. RELEVANCE: Is the artifact relevant to the task goal?\n\nFor artifact_type='cross_phase':\nValidate cross-phase consistency:\n{json.dumps(artifact, indent=2)}\n\nCheck for consistency between plan, execution steps, final answer, and memory artifacts.\nLook for contradictions, misalignments, or inconsistencies across phases.\n\n[Common ending for all artifact_types]\nReturn a JSON object with this structure:\n{\n  \"issues\": [\n    {\n      \"type\": \"specificity\" | \"relevance\" | \"consistency\" | \"hallucination\" | \"do_say_mismatch\",\n      \"severity\": \"LOW\" | \"MEDIUM\" | \"HIGH\" | \"CRITICAL\",\n      \"description\": \"Human-readable explanation of the issue\",\n      \"location\": {\"step_id\": \"...\", \"field\": \"...\"} (optional),\n      \"proposed_repair\": {\"field\": \"suggested fix\"} (optional)\n    }\n  ]\n}\n\nReturn only the JSON object, no explanation.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["artifact", "artifact_type", "tool_registry (optional)"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Template varies significantly by artifact_type (plan, step, execution_artifact, cross_phase). Each type has different validation criteria."
  },
  {
    "file": "aeon/convergence/engine.py",
    "function": "_get_system_prompt",
    "prompt_type": "system prompt",
    "category": "convergence assessment prompt",
    "exact_string": "You are a convergence assessment assistant. Evaluate task execution for completeness, coherence, and consistency.\nAssess whether the execution has converged on a complete, coherent, consistent solution.\nProvide numeric scores (0.0-1.0) for completeness and coherence, and alignment status for consistency.\nReturn structured JSON with scores, status, and explanations.",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "System prompt for convergence assessment"
  },
  {
    "file": "aeon/convergence/engine.py",
    "function": "_construct_convergence_prompt",
    "prompt_type": "user template",
    "category": "convergence assessment prompt",
    "exact_string": "Assess whether task execution has converged on a complete, coherent, consistent solution.\n\nPLAN STATE:\n{json.dumps(plan_state, indent=2)}\n\nEXECUTION RESULTS:\n{json.dumps(execution_results, indent=2)}\n\nSEMANTIC VALIDATION REPORT:\nIssues found: {len(semantic_validation_report.issues)}\nOverall severity: {semantic_validation_report.overall_severity}\n[Conditional: if issues exist]\nIssues:\n{issue_list (first 10 issues)}\n[End conditional]\n\nASSESSMENT CRITERIA:\n\n1. COMPLETENESS (0.0-1.0):\n   - Are all required steps completed?\n   - Is the task goal fully addressed?\n   - Are there any missing components or gaps?\n   - Threshold: {completeness_threshold:.2f}\n\n2. COHERENCE (0.0-1.0):\n   - Do the execution results form a coherent solution?\n   - Are the steps logically connected?\n   - Does the solution make sense as a whole?\n   - Threshold: {coherence_threshold:.2f}\n\n3. CONSISTENCY (alignment status):\n   - plan_aligned: Does execution align with the plan?\n   - step_aligned: Do step outputs align with step descriptions?\n   - answer_aligned: Does the final answer align with the task goal?\n   - memory_aligned: Are memory artifacts consistent with execution?\n   - Threshold: All alignment flags must be True\n\nReturn a JSON object with this structure:\n{\n  \"completeness_score\": 0.0-1.0,\n  \"coherence_score\": 0.0-1.0,\n  \"consistency_status\": {\n    \"plan_aligned\": true/false,\n    \"step_aligned\": true/false,\n    \"answer_aligned\": true/false,\n    \"memory_aligned\": true/false\n  },\n  \"detected_issues\": [\"issue description 1\", \"issue description 2\", ...],\n  \"reason_codes\": [\"reason_code_1\", \"reason_code_2\", ...],\n  \"metadata\": {\n    \"completeness_explanation\": \"explanation of completeness score\",\n    \"coherence_explanation\": \"explanation of coherence score\",\n    \"consistency_explanation\": \"explanation of consistency status\"\n  }\n}\n\nReturn only the JSON object, no explanation.",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["plan_state", "execution_results", "semantic_validation_report", "completeness_threshold", "coherence_threshold", "consistency_threshold"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Complex parameterized template for convergence assessment. Issue list is limited to first 10 issues."
  },
  {
    "file": "aeon/adaptive/heuristics.py",
    "function": "_get_task_profile_system_prompt",
    "prompt_type": "system prompt",
    "category": "task profile inference/update prompt",
    "exact_string": "You are a task complexity analyzer. Analyze tasks and infer their complexity characteristics.\nReturn only valid JSON with the required fields: reasoning_depth, information_sufficiency, expected_tool_usage, output_breadth, confidence_requirement, raw_inference.",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "System prompt for TaskProfile inference"
  },
  {
    "file": "aeon/adaptive/heuristics.py",
    "function": "_construct_task_profile_prompt",
    "prompt_type": "user template",
    "category": "task profile inference/update prompt",
    "exact_string": "Analyze this task and infer its complexity characteristics:\n\nTask: {task_description}\n\n[Conditional: if context provided]\nContext: {json.dumps(context, indent=2)}\n[End conditional]\n\nInfer the following dimensions:\n1. reasoning_depth: Integer 1-5 (1=very shallow, 2=shallow, 3=moderate, 4=deep, 5=very deep)\n2. information_sufficiency: Float 0.0-1.0 (0.0=insufficient, 1.0=sufficient)\n3. expected_tool_usage: One of \"none\", \"minimal\", \"moderate\", \"extensive\"\n4. output_breadth: One of \"narrow\", \"moderate\", \"broad\"\n5. confidence_requirement: One of \"low\", \"medium\", \"high\"\n6. raw_inference: Natural-language explanation of how each dimension was determined\n\nReturn a JSON object with these fields.\n",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["task_description", "context (optional)"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Template for TaskProfile inference"
  },
  {
    "file": "aeon/adaptive/heuristics.py",
    "function": "_get_update_task_profile_system_prompt",
    "prompt_type": "system prompt",
    "category": "task profile inference/update prompt",
    "exact_string": "You are a task complexity analyzer. Based on execution feedback (convergence failure, validation issues, blocked steps), update the TaskProfile to better reflect the actual task complexity.\nReturn only valid JSON with the required fields: reasoning_depth, information_sufficiency, expected_tool_usage, output_breadth, confidence_requirement, raw_inference.\nAdjust dimensions based on whether complexity was underestimated (increase) or overestimated (decrease).",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "System prompt for TaskProfile update"
  },
  {
    "file": "aeon/adaptive/heuristics.py",
    "function": "_construct_update_task_profile_prompt",
    "prompt_type": "user template",
    "category": "task profile inference/update prompt",
    "exact_string": "Update the TaskProfile based on execution feedback:\n\nCurrent TaskProfile:\n{json.dumps(current_profile.model_dump(), indent=2)}\n\nConvergence Assessment:\n{json.dumps(convergence_assessment, indent=2)}\n\nSemantic Validation Issues:\n{json.dumps(validation_issues, indent=2)}\n\nClarity States: {clarity_states}\n\nBased on this feedback, determine if the task complexity was underestimated or overestimated:\n- If complexity was underestimated (task harder than expected): Increase reasoning_depth, decrease information_sufficiency, increase expected_tool_usage, increase output_breadth, increase confidence_requirement\n- If complexity was overestimated (task easier than expected): Decrease reasoning_depth, increase information_sufficiency, decrease expected_tool_usage, decrease output_breadth, decrease confidence_requirement\n\nReturn an updated TaskProfile JSON with adjusted dimensions.\n",
    "is_reused": false,
    "is_parameterized": true,
    "parameters": ["current_profile", "convergence_assessment", "semantic_validation_report", "clarity_states"],
    "is_static": false,
    "prompt_id": null,
    "notes": "Template for TaskProfile update based on execution feedback"
  },
  {
    "file": "aeon/kernel/executor.py",
    "function": "execute_llm_reasoning_step",
    "prompt_type": "system prompt",
    "category": "execution reasoning prompt",
    "exact_string": "You are a reasoning assistant. Provide clear, structured responses. Include clarity_state and handoff_to_next in your response.",
    "is_reused": false,
    "is_parameterized": false,
    "is_static": true,
    "prompt_id": null,
    "notes": "Inline system prompt for LLM reasoning steps. Used with build_reasoning_prompt() user template."
  }
]
