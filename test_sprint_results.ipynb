{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 007 End-to-End Test\n",
    "\n",
    "This notebook tests the sprint results for prompt infrastructure, prompt contracts, and Phase E synthesis.\n",
    "\n",
    "**Assumptions:**\n",
    "- Live llama-cpp backend running on localhost:8000\n",
    "- OpenAI-compatible API endpoint at `http://localhost:8000/v1/chat/completions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "from IPython.display import display, Markdown, JSON\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '/home/brian/projects/Aeon-Architect')\n",
    "\n",
    "# Import Aeon components\n",
    "from aeon.llm.adapters.llama_cpp import LlamaCppAdapter\n",
    "from aeon.memory.kv_store import InMemoryKVStore\n",
    "from aeon.tools.registry import ToolRegistry\n",
    "from aeon.supervisor.repair import Supervisor\n",
    "from aeon.validation.schema import Validator\n",
    "from aeon.kernel.orchestrator import Orchestrator\n",
    "from aeon.kernel.state import OrchestrationState, ExecutionContext\n",
    "from aeon.plan.models import Plan\n",
    "from aeon.observability.logger import JSONLLogger\n",
    "from aeon.observability.helpers import generate_correlation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration\n",
    "LLM_API_URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "LLM_MODEL = \"llama-cpp-model\"\n",
    "\n",
    "# Execution Configuration\n",
    "TTL = 50  # Maximum cycles\n",
    "\n",
    "# Logging Configuration\n",
    "ENABLE_LOGGING = True\n",
    "LOG_FILE = None  # Set to a file path if you want to save logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing components...\")\n",
    "\n",
    "# Initialize LLM adapter for llama-cpp server\n",
    "llm = LlamaCppAdapter(\n",
    "    api_url=LLM_API_URL,\n",
    "    model=LLM_MODEL,\n",
    "    max_retries=3\n",
    ")\n",
    "print(f\"\u2713 LLM Adapter initialized: {LLM_API_URL}\")\n",
    "\n",
    "# Initialize memory\n",
    "memory = InMemoryKVStore()\n",
    "print(\"\u2713 Memory initialized\")\n",
    "\n",
    "# Initialize tool registry\n",
    "tool_registry = ToolRegistry()\n",
    "print(\"\u2713 Tool Registry initialized\")\n",
    "\n",
    "# Initialize supervisor for error repair\n",
    "supervisor = Supervisor(\n",
    "    llm_adapter=llm\n",
    ")\n",
    "print(\"\u2713 Supervisor initialized\")\n",
    "\n",
    "# Initialize validator\n",
    "validator = Validator()\n",
    "print(\"\u2713 Validator initialized\")\n",
    "\n",
    "# Initialize logger if enabled\n",
    "logger = None\n",
    "if ENABLE_LOGGING:\n",
    "    # JSONLLogger expects file_path as Optional[Path], not log_file\n",
    "    log_path = Path(LOG_FILE) if LOG_FILE else None\n",
    "    logger = JSONLLogger(file_path=log_path)\n",
    "    print(\"\u2713 Logger initialized\")\n",
    "\n",
    "print(\"\\nAll components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Orchestrator...\")\n",
    "\n",
    "orchestrator = Orchestrator(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    ttl=TTL,\n",
    "    tool_registry=tool_registry,\n",
    "    supervisor=supervisor,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "print(\"\u2713 Orchestrator created\")\n",
    "print(f\"  - TTL: {TTL} cycles\")\n",
    "print(f\"  - Memory: {'Enabled' if memory else 'Disabled'}\")\n",
    "print(f\"  - Tools: {len(tool_registry.list_all()) if tool_registry else 0} registered\")\n",
    "print(f\"  - Logging: {'Enabled' if logger else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define User Prompt\n",
    "\n",
    "Enter your test prompt below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER PROMPT - Modify this to test different scenarios\n",
    "user_prompt = \"Design a simple web application architecture with user authentication\"\n",
    "\n",
    "print(f\"User Prompt: {user_prompt}\")\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute End-to-End Test\n",
    "\n",
    "This will run the complete orchestration chain:\n",
    "- Phase A: TaskProfile & TTL allocation\n",
    "- Phase B: Initial Plan & Pre-Execution Refinement\n",
    "- Phase C: Execution Passes (Execute Batch \u2192 Evaluate \u2192 Decide \u2192 Refine)\n",
    "- Phase D: Adaptive Depth (TaskProfile updates)\n",
    "- Phase E: Answer Synthesis (Final Answer generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXECUTING END-TO-END TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Request: {user_prompt}\")\n",
    "print(f\"Start Time: {datetime.now().isoformat()}\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "# Track execution phases\n",
    "execution_log = {\n",
    "    \"phases\": [],\n",
    "    \"start_time\": datetime.now().isoformat(),\n",
    "    \"user_prompt\": user_prompt\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute multi-pass orchestration\n",
    "    result = orchestrator.execute_multipass(\n",
    "        request=user_prompt,\n",
    "        plan=None  # Let the system generate the plan\n",
    "    )\n",
    "    \n",
    "    execution_log[\"end_time\"] = datetime.now().isoformat()\n",
    "    execution_log[\"status\"] = \"success\"\n",
    "    execution_log[\"result\"] = result\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    execution_log[\"end_time\"] = datetime.now().isoformat()\n",
    "    execution_log[\"status\"] = \"error\"\n",
    "    execution_log[\"error\"] = str(e)\n",
    "    execution_log[\"error_type\"] = type(e).__name__\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EXECUTION FAILED: {type(e).__name__}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # Re-raise to see full traceback\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_log.get(\"status\") == \"success\":\n",
    "    result = execution_log[\"result\"]\n",
    "    \n",
    "    print(\"EXECUTION SUMMARY\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Status: {result.get('status', 'unknown')}\")\n",
    "    print(f\"Execution ID: {result.get('execution_id', 'N/A')}\")\n",
    "    \n",
    "    # Display execution history if available\n",
    "    if \"execution_history\" in result:\n",
    "        history = result[\"execution_history\"]\n",
    "        print(f\"\\nExecution History:\")\n",
    "        print(f\"  - Total Passes: {len(history.get('passes', []))}\")\n",
    "        \n",
    "        if \"overall_statistics\" in history:\n",
    "            stats = history[\"overall_statistics\"]\n",
    "            print(f\"  - Converged: {stats.get('convergence_achieved', 'N/A')}\")\n",
    "            print(f\"  - Total Refinements: {stats.get('total_refinements', 0)}\")\n",
    "    \n",
    "    # Display final answer if available (Phase E output)\n",
    "    if \"final_answer\" in result:\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINAL ANSWER (Phase E Synthesis)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if isinstance(final_answer, dict):\n",
    "            # Display answer text prominently\n",
    "            if \"answer_text\" in final_answer:\n",
    "                print(\"\\nAnswer:\")\n",
    "                print(\"-\" * 80)\n",
    "                print(final_answer[\"answer_text\"])\n",
    "                print(\"-\" * 80)\n",
    "            \n",
    "            # Display confidence if present\n",
    "            if \"confidence\" in final_answer and final_answer[\"confidence\"] is not None:\n",
    "                print(f\"\\nConfidence: {final_answer['confidence']}\")\n",
    "            \n",
    "            # Display TTL exhaustion warning if present\n",
    "            if final_answer.get(\"ttl_exhausted\", False):\n",
    "                print(\"\\n\u26a0\ufe0f  WARNING: TTL was exhausted during execution\")\n",
    "            \n",
    "            # Display notes if present\n",
    "            if \"notes\" in final_answer and final_answer[\"notes\"]:\n",
    "                print(f\"\\nNotes: {final_answer['notes']}\")\n",
    "            \n",
    "            # Display used step IDs if present\n",
    "            if \"used_step_ids\" in final_answer and final_answer[\"used_step_ids\"]:\n",
    "                print(f\"\\nUsed Step IDs: {', '.join(final_answer['used_step_ids'])}\")\n",
    "            \n",
    "            # Display metadata summary\n",
    "            if \"metadata\" in final_answer and final_answer[\"metadata\"]:\n",
    "                print(\"\\nMetadata:\")\n",
    "                for key, value in final_answer[\"metadata\"].items():\n",
    "                    print(f\"  - {key}: {value}\")\n",
    "        else:\n",
    "            print(final_answer)\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  No final answer available in result\")\n",
    "    \n",
    "    # Display timing information\n",
    "    start_time = datetime.fromisoformat(execution_log[\"start_time\"])\n",
    "    end_time = datetime.fromisoformat(execution_log[\"end_time\"])\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"Execution Duration: {duration:.2f} seconds\")\n",
    "    print(f\"Start: {execution_log['start_time']}\")\n",
    "    print(f\"End: {execution_log['end_time']}\")\n",
    "else:\n",
    "    print(\"Execution failed. See error details above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Execution Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_log.get(\"status\") == \"success\":\n",
    "    result = execution_log[\"result\"]\n",
    "    \n",
    "    print(\"DETAILED EXECUTION LOG\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display execution history with passes\n",
    "    if \"execution_history\" in result:\n",
    "        history = result[\"execution_history\"]\n",
    "        passes = history.get(\"passes\", [])\n",
    "        \n",
    "        print(f\"\\nTotal Execution Passes: {len(passes)}\")\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        \n",
    "        for i, pass_data in enumerate(passes, 1):\n",
    "            print(f\"\\nPass {i}:\")\n",
    "            print(f\"  - Pass Number: {pass_data.get('pass_number', 'N/A')}\")\n",
    "            print(f\"  - Phase: {pass_data.get('phase', 'N/A')}\")\n",
    "            print(f\"  - TTL Remaining: {pass_data.get('ttl_remaining', 'N/A')}\")\n",
    "            \n",
    "            if \"timing_information\" in pass_data:\n",
    "                timing = pass_data[\"timing_information\"]\n",
    "                print(f\"  - Start Time: {timing.get('start_time', 'N/A')}\")\n",
    "                if \"end_time\" in timing:\n",
    "                    print(f\"  - End Time: {timing.get('end_time', 'N/A')}\")\n",
    "            \n",
    "            if \"plan_state\" in pass_data:\n",
    "                plan_state = pass_data[\"plan_state\"]\n",
    "                if isinstance(plan_state, dict):\n",
    "                    print(f\"  - Plan Goal: {plan_state.get('goal', 'N/A')}\")\n",
    "                    steps = plan_state.get(\"steps\", [])\n",
    "                    print(f\"  - Steps Count: {len(steps)}\")\n",
    "                    for step in steps[:3]:  # Show first 3 steps\n",
    "                        step_id = step.get(\"step_id\", \"unknown\")\n",
    "                        status = step.get(\"status\", \"unknown\")\n",
    "                        desc = step.get(\"description\", \"\")[:50]\n",
    "                        print(f\"    \u2022 Step {step_id} [{status}]: {desc}...\")\n",
    "                    if len(steps) > 3:\n",
    "                        print(f\"    ... and {len(steps) - 3} more steps\")\n",
    "            \n",
    "            if \"execution_results\" in pass_data:\n",
    "                exec_results = pass_data[\"execution_results\"]\n",
    "                if exec_results:\n",
    "                    print(f\"  - Execution Results: {len(exec_results)} items\")\n",
    "            \n",
    "            if \"evaluation_results\" in pass_data:\n",
    "                eval_results = pass_data[\"evaluation_results\"]\n",
    "                if eval_results:\n",
    "                    print(f\"  - Evaluation Results: Present\")\n",
    "                    if isinstance(eval_results, dict):\n",
    "                        if \"convergence_assessment\" in eval_results:\n",
    "                            conv = eval_results[\"convergence_assessment\"]\n",
    "                            if isinstance(conv, dict):\n",
    "                                print(f\"    \u2022 Converged: {conv.get('converged', 'N/A')}\")\n",
    "            \n",
    "            if \"refinement_changes\" in pass_data:\n",
    "                refinements = pass_data[\"refinement_changes\"]\n",
    "                if refinements:\n",
    "                    print(f\"  - Refinements: {len(refinements) if isinstance(refinements, list) else 'Present'}\")\n",
    "    \n",
    "    # Display overall statistics\n",
    "    if \"execution_history\" in result and \"overall_statistics\" in result[\"execution_history\"]:\n",
    "        stats = result[\"execution_history\"][\"overall_statistics\"]\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"OVERALL STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Execution failed. No detailed log available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Result JSON (for Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_log.get(\"status\") == \"success\":\n",
    "    result = execution_log[\"result\"]\n",
    "    \n",
    "    # Convert result to JSON-serializable format\n",
    "    def make_json_serializable(obj):\n",
    "        \"\"\"Recursively convert objects to JSON-serializable format.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [make_json_serializable(item) for item in obj]\n",
    "        elif hasattr(obj, 'model_dump'):\n",
    "            return obj.model_dump()\n",
    "        elif hasattr(obj, 'dict'):\n",
    "            return obj.dict()\n",
    "        elif isinstance(obj, (str, int, float, bool, type(None))):\n",
    "            return obj\n",
    "        else:\n",
    "            return str(obj)\n",
    "    \n",
    "    serializable_result = make_json_serializable(result)\n",
    "    \n",
    "    print(\"FULL RESULT JSON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(json.dumps(serializable_result, indent=2, default=str))\n",
    "    \n",
    "    # Optionally save to file\n",
    "    # with open('execution_result.json', 'w') as f:\n",
    "    #     json.dump(serializable_result, f, indent=2, default=str)\n",
    "    # print(\"\\n\u2713 Result saved to execution_result.json\")\n",
    "else:\n",
    "    print(\"Execution failed. No result to display.\")\n",
    "    if \"error\" in execution_log:\n",
    "        print(f\"\\nError Details:\")\n",
    "        print(json.dumps({\n",
    "            \"error_type\": execution_log.get(\"error_type\"),\n",
    "            \"error\": execution_log.get(\"error\")\n",
    "        }, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase E (Answer Synthesis) Details\n",
    "\n",
    "This section shows the Phase E output in detail, which is the key deliverable from Sprint 007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execution_log.get(\"status\") == \"success\":\n",
    "    result = execution_log[\"result\"]\n",
    "    \n",
    "    if \"final_answer\" in result:\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        \n",
    "        # Convert to dict if it's a Pydantic model\n",
    "        if hasattr(final_answer, 'model_dump'):\n",
    "            final_answer = final_answer.model_dump()\n",
    "        elif hasattr(final_answer, 'dict'):\n",
    "            final_answer = final_answer.dict()\n",
    "        \n",
    "        print(\"PHASE E: ANSWER SYNTHESIS DETAILS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nThis is the output from Phase E (Answer Synthesis), which consolidates\")\n",
    "        print(\"execution results, plan state, and convergence information into a\")\n",
    "        print(\"coherent final answer.\")\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        \n",
    "        print(\"\\nFull FinalAnswer Structure:\")\n",
    "        print(json.dumps(final_answer, indent=2, default=str))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"VALIDATION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Validate FinalAnswer structure\n",
    "        required_fields = [\"answer_text\", \"metadata\"]\n",
    "        optional_fields = [\"confidence\", \"used_step_ids\", \"notes\", \"ttl_exhausted\"]\n",
    "        \n",
    "        print(\"\\nRequired Fields:\")\n",
    "        for field in required_fields:\n",
    "            status = \"\u2713\" if field in final_answer else \"\u2717\"\n",
    "            value = final_answer.get(field, \"MISSING\")\n",
    "            print(f\"  {status} {field}: {type(value).__name__}\")\n",
    "        \n",
    "        print(\"\\nOptional Fields:\")\n",
    "        for field in optional_fields:\n",
    "            if field in final_answer:\n",
    "                value = final_answer[field]\n",
    "                print(f\"  \u2713 {field}: {value}\")\n",
    "            else:\n",
    "                print(f\"  - {field}: Not present\")\n",
    "        \n",
    "        print(\"\\nMetadata Fields:\")\n",
    "        if \"metadata\" in final_answer and isinstance(final_answer[\"metadata\"], dict):\n",
    "            for key, value in final_answer[\"metadata\"].items():\n",
    "                print(f\"  - {key}: {value}\")\n",
    "        else:\n",
    "            print(\"  - No metadata present\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No final_answer found in result.\")\n",
    "        print(\"\\nThis may indicate:\")\n",
    "        print(\"  1. Phase E was not executed\")\n",
    "        print(\"  2. Phase E execution failed\")\n",
    "        print(\"  3. Result structure is different than expected\")\n",
    "        print(\"\\nAvailable keys in result:\")\n",
    "        for key in result.keys():\n",
    "            print(f\"  - {key}\")\n",
    "else:\n",
    "    print(\"Execution failed. Cannot display Phase E details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Registry Audit\n",
    "\n",
    "Verify that prompts are being retrieved from the centralized registry (Sprint 007 User Story 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.prompts.registry import get_prompt_registry, PromptId\n",
    "\n",
    "print(\"PROMPT REGISTRY AUDIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "registry = get_prompt_registry()\n",
    "all_prompts = registry.list_prompts()\n",
    "\n",
    "print(f\"\\nTotal Prompts in Registry: {len(all_prompts)}\")\n",
    "print(\"\\nAvailable Prompt IDs:\")\n",
    "for prompt_id in sorted(all_prompts, key=lambda x: x.value):\n",
    "    print(f\"  - {prompt_id.value}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Key Prompts for Sprint 007:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check for Phase E prompts (User Story 3)\n",
    "phase_e_prompts = [\n",
    "    PromptId.ANSWER_SYNTHESIS_SYSTEM,\n",
    "    PromptId.ANSWER_SYNTHESIS_USER\n",
    "]\n",
    "\n",
    "for prompt_id in phase_e_prompts:\n",
    "    if prompt_id in all_prompts:\n",
    "        print(f\"\u2713 {prompt_id.value}: Registered\")\n",
    "        try:\n",
    "            # Try to get the prompt definition\n",
    "            prompt_def = registry._registry.get(prompt_id)\n",
    "            if prompt_def:\n",
    "                has_input = prompt_def.input_model is not None\n",
    "                has_output = prompt_def.output_model is not None\n",
    "                print(f\"    - Input Model: {'\u2713' if has_input else '\u2717'}\")\n",
    "                print(f\"    - Output Model: {'\u2713' if has_output else '\u2717'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error checking definition: {e}\")\n",
    "    else:\n",
    "        print(f\"\u2717 {prompt_id.value}: NOT REGISTERED\")\n",
    "\n",
    "print(\"\\n\u2713 Prompt registry audit complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}