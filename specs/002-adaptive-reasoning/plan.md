# Implementation Plan: Sprint 2 - Adaptive Reasoning Engine

**Branch**: `002-adaptive-reasoning` | **Date**: 2025-01-27 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/002-adaptive-reasoning/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Sprint 2 transforms Aeon from a deterministic single-pass orchestrator into a recursive, adaptive, semantically-aware intelligent reasoning engine with five Tier-1 capabilities: Multi-Pass Execution Loop, Recursive Planning & Re-Planning, Convergence Engine, Adaptive Depth Heuristics, and Semantic Validation Layer.

**Prerequisite**: Before Sprint 2 implementation begins, a mandatory kernel refactoring must be completed to reduce combined LOC of `orchestrator.py` and `executor.py` from 786 to below 700 LOC.

## Master Constraint: LLM-Based Reasoning Requirement

**All semantic, cognitive, or evaluative functionality MUST be implemented using LLM-based reasoning.**

Heuristic or lexical methods (including static word lists, simple regex patterns, keyword matching, or rule-based logic) MAY be used only as secondary signals to inform LLM reasoning inputs and MUST NOT serve as primary decision-making mechanisms.

This constraint applies to all User Stories and Functional Requirements that involve:
- Semantic analysis (specificity, relevance, consistency detection)
- Cognitive assessment (complexity evaluation, ambiguity detection, information sufficiency)
- Evaluative functions (convergence determination, quality scoring, issue classification)

**Rationale**: To ensure the system leverages the full reasoning capabilities of LLMs rather than relying on brittle pattern-matching approaches that cannot handle nuanced, context-dependent semantic understanding.

## Algorithmic Guidance Principles

### 1. Multi-Pass Loop - Algorithmic Guidance

* **Refinement MUST be driven by structured LLM feedback only.**
  - Refinement decisions (what to refine, how to refine) MUST be determined by LLM analysis of validation reports and convergence assessments
  - Heuristic-based refinement triggers (e.g., "if issue count > threshold, refine") are NOT allowed as primary mechanisms

* **The loop MUST treat the LLM as the cognitive core, not heuristics.**
  - Phase transitions (plan → execute → evaluate → refine) MUST be driven by LLM-based evaluation
  - Convergence detection MUST use LLM reasoning, not rule-based checks

* **Refinement MUST be targeted (modify only steps flagged by validator, not wholesale regeneration).**
  - Refinement operations MUST identify specific plan fragments or steps to modify based on LLM analysis
  - Full-plan regeneration is NOT allowed unless LLM determines it necessary

* **No heuristic-based convergence shortcuts are allowed.**
  - Convergence MUST be determined by LLM-based completeness/coherence/consistency assessment
  - Simple checks like "no validation issues" or "all steps complete" are NOT sufficient for convergence

### 2. Recursive Planner - Algorithmic Guidance

* **Subplans MUST be generated by LLM decomposition, not heuristic splitting.**
  - Complex step decomposition MUST use LLM reasoning to understand step complexity and generate appropriate subplans
  - Heuristic splitting (e.g., "split if step description > N words") is NOT allowed

* **Refinements MUST be delta-style edits, not full-plan rewrites.**
  - Refinement operations MUST produce structured edit operations (add, modify, remove) targeting specific plan fragments
  - Full-plan regeneration is NOT allowed unless LLM determines it necessary

* **Planner MUST output a structured graph of sub-steps with rationale.**
  - Subplan generation MUST include LLM-generated rationale explaining why decomposition was needed
  - Subplan structure MUST be represented as a graph with parent-child relationships

* **Planner MUST preserve stable IDs between passes.**
  - Step IDs MUST remain stable across refinement operations unless LLM determines ID changes are necessary
  - ID stability enables tracking of step evolution across passes

* **Planner MUST produce explainable changes ("why these edits").**
  - Each refinement operation MUST include LLM-generated explanation of why specific changes were made
  - Explanations MUST reference validation issues, convergence failures, or other semantic concerns

* **Planner MUST consider validator issues as constraints, not suggestions.**
  - Validation issues MUST be treated as hard constraints that MUST be addressed
  - Planner refinements MUST directly address validation issues identified by semantic validator

### 3. Semantic Validation - Algorithmic Guidance

* **Primary validation MUST be produced by LLM reasoning.**
  - All semantic validation (specificity, relevance, do/say mismatch, hallucination, consistency) MUST use LLM-based analysis
  - Validation reports MUST be generated through LLM reasoning, not pattern matching
  - **PROHIBITED**: No validator module may implement lists of weak verbs, vague phrases, or other lexical indicators as decision logic. Any such indicators, if used, MUST only be passed as context to the LLM and MUST NOT serve as primary decision-making mechanisms.

* **All scoring, typing, and repair generation MUST be LLM-delegated.**
  - All severity scoring, issue typing, and repair suggestions MUST be delegated to the LLM through structured JSON calls
  - **PROHIBITED**: No deterministic scoring or classification logic may exist in code. Severity, issue types, and repair suggestions MUST be generated entirely by LLM reasoning.
  - The validator MUST NOT maintain threshold-based severity mappings, keyword-based issue classification rules, or template-based repair generation.

* **Structural validation is strictly limited to malformed data detection.**
  - Structural validation MAY detect only:
    - duplicate IDs
    - missing required fields
    - invalid schema shapes (e.g., wrong data types, missing nested structures)
  - **PROHIBITED**: Structural checks MUST NOT influence semantic-level decisions. Structural validation results MUST be passed to the LLM as context only, never used to determine semantic validity independently.

* **Invalid JSON/schema violations MUST trigger LLM retry, not heuristic fixes.**
  - If the LLM returns invalid JSON or schema-noncompliant output, the validator MUST retry with a corrective system prompt
  - **PROHIBITED**: The validator MUST NOT attempt to fix output heuristically (e.g., by parsing partial JSON, inferring missing fields, or mapping loosely-typed values)
  - The retry mechanism MUST explicitly instruct the LLM about the schema violation and request corrected structured output

* **All internal "hint signals" are LLM context only.**
  - All internal signals (ambiguity flags, missing-field counts, malformed patterns, lexical indicators) MUST be passed to the LLM as part of evaluation context
  - **PROHIBITED**: These signals MUST NEVER be used as standalone decision-makers. They may inform LLM reasoning but cannot determine validation outcomes independently.

* **Specificity, relevance, coherence, consistency MUST be evaluated semantically via LLM reasoning.**
  - Step specificity assessment MUST use LLM to determine if steps are concrete and actionable
  - Logical relevance MUST use LLM to determine if steps contribute to goal achievement
  - Coherence and consistency MUST use LLM to detect contradictions and logical gaps

* **Validation MUST produce deterministic JSON outputs parsed into ValidationIssue.**
  - LLM validation outputs MUST be structured JSON conforming to ValidationIssue schema
  - Validation reports MUST be parseable into ValidationIssue objects for programmatic processing
  - All outputs MUST strictly match schemas defined in data-model.md and the specification

### 4. Convergence Engine - Algorithmic Guidance

* **Convergence criteria MUST be judged by LLM-based explanation.**
  - Completeness, coherence, and consistency assessments MUST use LLM reasoning to evaluate execution artifacts
  - Convergence determination MUST be based on LLM-generated explanations, not simple rule checks

* **All scoring and classification MUST be LLM-delegated.**
  - All completeness scores, coherence scores, consistency assessments, and reason code generation MUST be delegated to the LLM through structured JSON calls
  - **PROHIBITED**: No deterministic scoring or classification logic may exist in code. Scores and reason codes MUST be generated entirely by LLM reasoning.
  - The convergence engine MUST NOT maintain threshold-based scoring logic, rule-based reason code mappings, or heuristic convergence determination

* **Completeness/coherence/consistency MUST use LLM-internal reasoning, not lexical-fuzzy matching.**
  - Completeness checks MUST use LLM to determine if all goals are addressed and no steps are missing
  - Coherence checks MUST use LLM to detect contradictions and logical inconsistencies
  - Consistency checks MUST use LLM to verify alignment between plan, steps, and final answer

* **LLM MUST return a structured reason-code set for convergence.**
  - Convergence assessments MUST include structured reason codes (e.g., "complete", "incoherent", "missing_step", "contradiction_detected")
  - Reason codes MUST be generated by LLM reasoning, not mapped from heuristic checks

* **All internal signals are LLM context only.**
  - Internal signals (keyword overlap, empty-issue lists, validation issue counts, structural patterns) MUST be passed to the LLM as part of evaluation context
  - **PROHIBITED**: These signals MUST NEVER be used as standalone decision-makers for convergence determination. They may inform LLM reasoning but cannot determine convergence outcomes independently.

* **Convergence MUST NEVER be calculated via:**
  - keyword overlap
  - empty-issue lists
  - rule-based scoring only
  - These methods MAY be used only as secondary signals to inform LLM inputs

* **Default Convergence Criteria (used as LLM context only):**
  - Default thresholds (completeness >= 0.95, coherence >= 0.90, consistency >= 0.90) MUST be provided to the LLM as guidance context
  - The LLM MUST evaluate against these thresholds, but the final convergence determination and scoring MUST be generated by LLM reasoning
  - **PROHIBITED**: Default criteria MUST NOT be enforced deterministically in code - they are informational signals for the LLM, not hard rules

* **Schema enforcement applies to convergence outputs.**
  - All ConvergenceAssessment outputs MUST strictly match schemas defined in data-model.md and the specification
  - Invalid JSON or schema violations MUST trigger retry with corrective system prompt (NOT heuristic fixes)

### 5. Adaptive Depth - Algorithmic Guidance

* **Depth decisions MUST use LLM-based "task complexity self-assessment."**
  - TaskProfile inference MUST use LLM reasoning to assess task complexity across all dimensions
  - Reasoning depth and TTL allocation MUST be determined by LLM analysis of TaskProfile dimensions

* **TTL increases/decreases MUST be determined by LLM analysis of:**
  - ambiguity (detected through LLM semantic analysis)
  - uncertainty (detected through LLM assessment of information sufficiency)
  - dependency depth (detected through LLM analysis of step relationships)
  - reasoning complexity (detected through LLM assessment of cognitive load)

* **No "word count," "token length," or heuristic approximations allowed.**
  - TTL allocation MUST NOT be based on simple metrics like word count or token length
  - Complexity assessment MUST use LLM reasoning to understand semantic complexity, not surface-level metrics

### 6. Execution History - Algorithmic Guidance

* **MUST store semantic validator output.**
  - Execution history MUST include complete semantic validation reports (ValidationIssue objects) for each pass
  - Validation reports MUST be stored as structured data, not free-form text

* **MUST store planner deltas.**
  - Execution history MUST include structured refinement operations (add, modify, remove) with before/after states
  - Planner deltas MUST include LLM-generated explanations for each change

* **MUST store convergence explanations.**
  - Execution history MUST include LLM-generated convergence assessments with reason codes and explanations
  - Convergence metadata MUST be structured (not free-form text) for programmatic analysis

* **MUST store adaptive-depth decisions.**
  - Execution history MUST include TaskProfile snapshots and LLM-generated adjustment reasons
  - Adaptive depth decisions MUST be stored as structured data with rationale

* **MUST be fully structured, not just log text.**
  - Execution history MUST be stored as structured JSON/YAML with well-defined schemas
  - All semantic outputs (validation, convergence, refinement) MUST be parseable into structured objects

## Mandatory Kernel Refactoring Phase (Prerequisite)

**Status**: Required before any Sprint 2 User Stories implementation  
**Current LOC**: 786 lines (522 orchestrator.py + 264 executor.py)  
**Target LOC**: <700 lines (combined)  
**Constitutional Limit**: 800 LOC (providing 100+ LOC headroom)

### Refactoring Objectives

1. **LOC Reduction**: Measure and reduce combined LOC of `orchestrator.py` and `executor.py` to below 700 LOC
2. **Structural Refactoring Only**: No behavior changes, no interface changes - pure structural reorganization
3. **Logic Extraction**: Move non-orchestration logic to appropriate external modules
4. **Behavioral Preservation**: Maintain 100% Sprint 1 behavioral compatibility

### Refactoring Constraints

- **Structural-Only**: Refactoring MUST be structural only — no behavior changes, no interface changes
- **Logic Extraction**: Any non-orchestration logic found in `orchestrator.py` or `executor.py` MUST be moved into appropriate external modules
- **Supporting Modules**: Supporting kernel modules (e.g., `state.py`) may contain only pure data structures and simple containers, and may NOT be used to circumvent kernel LOC limits
- **Regression Testing**: All existing Sprint 1 tests MUST pass without modification after refactoring
- **No Behavioral Drift**: Sprint 1 behavior MUST remain identical - verified through comprehensive regression test suite

### Refactoring Approach

1. **Analysis Phase**:
   - Audit `orchestrator.py` and `executor.py` to identify non-orchestration logic
   - Identify extractable functions, classes, or modules that can be moved externally
   - Measure current LOC and identify reduction targets

2. **Extraction Phase**:
   - Move non-orchestration logic to appropriate external modules (tools, supervisors, utilities)
   - Ensure extracted code maintains clean interfaces
   - Preserve all existing functionality and interfaces

3. **Validation Phase**:
   - Run full regression test suite to confirm no behavioral changes
   - Verify LOC is below 700 (combined)
   - Confirm all interfaces remain unchanged
   - Document refactoring changes

### Success Criteria

- [ ] Combined LOC of `orchestrator.py` and `executor.py` is below 700 lines
- [ ] All Sprint 1 regression tests pass without modification
- [ ] No interface changes (all existing code using kernel continues to work)
- [ ] No behavioral changes (execution results identical to Sprint 1)
- [ ] Refactoring documented with before/after LOC measurements

### Blocking Condition

**This refactoring MUST be completed and validated before any Sprint 2 functional requirements are implemented.**

## Technical Context

**Language/Version**: Python 3.12  
**Primary Dependencies**: pydantic >= 2.0.0, jsonschema >= 4.0.0, requests >= 2.31.0, pyyaml >= 6.0.0
**Storage**: File storage: JSONL logs and YAML config files and In-memory storage: InMemoryKVStore (not persistent)
**Testing**: pytest
**Target Platform**: Linux x86_64 (Ubuntu 22.04 or later)
**Project Type**: simple CLI wrapper over a scalable backend for future web and mobile frontends
**Performance Goals**: 95% of tasks whose initial TaskProfile indicates high reasoning_depth or low information_sufficiency completed within 5 minutes, average passes per task <= 5, convergence engine evaluation completing within 250ms
**Constraints**: 800 line Kernel LOC limit, no domain logic in the Kernel, all semantic, planning, refinement and convergence logic MUST live in external modules. All semantic, cognitive, and evaluative functionality MUST use LLM-based reasoning as primary mechanism.
**Scale/Scope**: single-node local execution, 1-5 tasks per request expected, combined input/output of 2k-8k tokens is typical task size.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**Prerequisite**: Mandatory kernel refactoring (Phase -1) must be completed first to bring kernel LOC below 700 before Sprint 2 implementation begins.

**Kernel Minimalism (Principle I)**: 
- [x] Does this feature add code to the kernel? If yes, justify why it cannot be a tool/supervisor.
  - **Answer**: No. All Sprint 2 features (multi-pass loop, recursive planning, convergence engine, adaptive depth, semantic validation) are implemented as external modules. The kernel orchestrator will invoke these modules through clean interfaces but will not contain their logic.
- [x] Will kernel LOC remain under 800 after this feature? (Prerequisite: Must be <700 after refactoring)
  - **Answer**: Yes. Kernel refactoring reduces LOC to <700, and Sprint 2 features are external modules. Kernel changes are limited to interface invocations, not feature implementation.
- [x] Does this feature add domain logic to the kernel? (MUST be NO)
  - **Answer**: No. All semantic, cognitive, and evaluative logic lives in external modules (semantic validator, convergence engine, recursive planner, adaptive depth heuristics).

**Separation of Concerns (Principle II)**:
- [x] Are new capabilities implemented as tools/supervisors, not kernel changes?
  - **Answer**: Yes. Multi-pass loop coordination is minimal kernel logic (phase transitions). Semantic validation, convergence engine, recursive planner, and adaptive depth are external modules.
- [x] Do new modules interact through clean interfaces only?
  - **Answer**: Yes. All new modules (semantic validator, convergence engine, recursive planner, adaptive depth) expose well-defined interfaces. Kernel invokes these interfaces but does not access internals.
- [x] Are kernel internals accessed by external modules? (MUST be NO)
  - **Answer**: No. External modules interact with kernel only through defined interfaces (orchestrator methods, state accessors). No direct kernel internal access.

**Declarative Plans (Principle III)**:
- [x] If this feature affects plans, are they JSON/YAML declarative structures?
  - **Answer**: Yes. Plans remain JSON/YAML declarative structures. Refinements produce delta-style edits that preserve declarative nature.
- [x] Is any procedural logic added to plans? (MUST be NO)
  - **Answer**: No. Plans remain pure data structures. Refinements modify plan data, not add executable code.

**Extensibility (Principle IX)**:
- [x] Can this feature be added without kernel mutation?
  - **Answer**: Minimal kernel changes required for multi-pass loop coordination (phase transitions, pass management). Core feature logic is external.
- [x] If kernel changes are required, are they rare, deliberate, and documented?
  - **Answer**: Yes. Kernel changes are limited to orchestrator phase management and interface invocations. All changes are deliberate and documented.

**Sprint 1 Scope (Principle X)**:
- [x] Is this feature within Sprint 1 scope? (No diagrams, IaC, RAG, cloud logic, embeddings, multi-agent, advanced memory)
  - **Answer**: No, this is Sprint 2. Sprint 2 explicitly extends Sprint 1 with multi-pass execution, recursive planning, convergence detection, adaptive depth, and semantic validation. These are Tier-1 Sprint 2 capabilities.

## Project Structure

### Documentation (this feature)

```text
specs/002-adaptive-reasoning/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (/speckit.plan command)
├── data-model.md        # Phase 1 output (/speckit.plan command)
├── quickstart.md         # Phase 1 output (/speckit.plan command)
├── contracts/           # Phase 1 output (/speckit.plan command)
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```text
aeon/
├── kernel/
│   ├── orchestrator.py  # Multi-pass loop coordination (minimal changes)
│   ├── executor.py       # Step execution (unchanged from Sprint 1)
│   └── state.py          # State management (unchanged)
├── validation/
│   ├── semantic_models.py      # ValidationIssue, SemanticValidationReport models
│   ├── semantic_interface.py   # Semantic validation interface
│   └── semantic_validator.py   # LLM-based semantic validation implementation
├── convergence/
│   ├── models.py         # ConvergenceAssessment model
│   ├── interface.py      # Convergence engine interface
│   └── engine.py         # LLM-based convergence engine implementation
├── plan/
│   ├── recursive_models.py     # Subplan model
│   ├── recursive_interface.py   # Recursive planner interface
│   └── recursive_planner.py    # LLM-based recursive planner implementation
├── adaptive/
│   ├── models.py         # AdaptiveDepthConfiguration, TaskProfile models
│   ├── interface.py     # Adaptive depth interface
│   └── heuristics.py    # LLM-based adaptive depth heuristics
└── kernel/
    ├── multipass_models.py      # ExecutionPass, RefinementAction models
    ├── history_models.py        # ExecutionHistory model
    ├── history_interface.py     # Execution history interface
    └── history_recorder.py      # Execution history recorder

tests/
├── contract/
├── integration/
│   └── test_multipass_integration.py  # Multi-pass execution integration tests
└── unit/
    ├── validation/
    │   └── test_semantic_validator.py
    ├── convergence/
    │   └── test_convergence_engine.py
    ├── plan/
    │   └── test_recursive_planner.py
    └── adaptive/
        └── test_adaptive_depth.py
```

**Structure Decision**: Single Python package structure. All Sprint 2 features are external modules that integrate with existing kernel through clean interfaces. Kernel remains minimal (<700 LOC after refactoring).

## Implementation Phases

### Phase -1: Mandatory Kernel Refactoring (PREREQUISITE)

**Status**: Blocking - Must be completed before any Sprint 2 work begins

- Measure current LOC (orchestrator.py + executor.py)
- Identify and extract non-orchestration logic to external modules
- Reduce combined LOC to below 700 lines
- Validate no behavioral changes through regression testing
- Document refactoring changes

**Deliverables**:
- Refactored kernel with LOC < 700
- Regression test results confirming no behavioral drift
- Refactoring documentation

### Phase 0: Research & Design

**Prerequisites**: Phase -1 complete, kernel LOC < 700

- Research LLM-based semantic validation patterns
- Research LLM-based convergence detection approaches
- Research LLM-based recursive planning strategies
- Research LLM-based adaptive depth heuristics
- Design structured output formats for LLM reasoning (JSON schemas for ValidationIssue, ConvergenceAssessment, etc.)
- Design delta-style refinement operations
- Design execution history data structures

**Deliverables**: `research.md`

### Phase 1: Design & Contracts

**Prerequisites**: Phase 0 complete

- Define data models for execution passes, convergence assessments, semantic validation reports, TaskProfile, etc.
- Generate API contracts for new interfaces (semantic validator, convergence engine, recursive planner, adaptive depth)
- Create quickstart documentation
- Update agent context

**Deliverables**: `data-model.md`, `contracts/`, `quickstart.md`

#### Structured Output Schemas for LLM Reasoning
(ValidationIssue, SemanticValidationReport, ConvergenceAssessment, TaskProfile, AdaptiveDepthDecision)

#### Schema Enforcement Checkpoints

**Critical Requirement**: All models produced by the LLM must strictly match the schemas defined in `data-model.md` and the specification. The following checkpoints MUST be implemented:

1. **Pre-LLM Schema Definition Checkpoint** (Phase 1):
   - All schemas for LLM outputs MUST be defined in `data-model.md` with explicit field types, constraints, and required/optional fields
   - Schemas MUST be validated against JSON Schema or Pydantic model definitions
   - Documentation MUST clearly specify which fields are required vs optional for each schema

2. **Post-LLM Output Validation Checkpoint** (Implementation):
   - After receiving LLM output, validators MUST parse and validate against the schema using strict validation (no coercion, no type inference)
   - Invalid JSON MUST trigger retry with corrective system prompt (NOT heuristic fixes)
   - Schema-noncompliant output (missing required fields, wrong types, extra fields not in schema) MUST trigger retry with corrective prompt
   - All validation errors MUST be reported to the LLM in the retry prompt with specific field-level violations

3. **Runtime Schema Enforcement Checkpoint** (Implementation):
   - All LLM response parsers MUST use strict schema validation before processing
   - Schema violations MUST NOT be silently corrected or ignored
   - Failed schema validation MUST result in retry, not fallback to partial parsing or heuristic mapping

### Phase 2: Implementation Tasks

**Prerequisites**: Phase 1 complete

- Break down implementation into specific tasks
- Define task dependencies and sequencing
- Ensure all tasks enforce LLM-only semantic validation constraints (see §3 "Semantic Validation - Algorithmic Guidance")
- Ensure all tasks enforce LLM-only convergence scoring constraints (see §4 "Convergence Engine - Algorithmic Guidance")
- Ensure all tasks implement schema enforcement checkpoints (see Phase 1 "Schema Enforcement Checkpoints")

**Implementation Constraints** (MUST be enforced in all implementation tasks):

1. **No Lexical/Keyword-Based Decision Logic**: Validators MUST NOT implement lists of weak verbs, vague phrases, or lexical indicators as decision logic. All such indicators must be LLM context only.

2. **No Deterministic Scoring/Classification**: Severity scoring, issue typing, and repair suggestions MUST be LLM-delegated. No deterministic logic may determine these outcomes.

3. **Structural Validation Scope Limited**: Structural validation may only detect malformed data (duplicate IDs, missing fields, invalid schema shapes). These checks MUST NOT influence semantic-level decisions.

4. **LLM Retry for Invalid Output**: Invalid JSON or schema violations MUST trigger LLM retry with corrective prompt, NOT heuristic fixes.

5. **Strict Schema Adherence**: All LLM outputs MUST match schemas from data-model.md. Schema enforcement checkpoints MUST be implemented at all LLM output parsing points.

6. **Hint Signals are Context Only**: All internal signals (ambiguity flags, missing-field counts, malformed patterns) MUST be passed to LLM as evaluation context, never as standalone decision-makers.

**Deliverables**: `tasks.md` (created by `/speckit.tasks` command)

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| N/A | N/A | N/A |
